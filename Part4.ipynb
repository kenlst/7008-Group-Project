{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e14a01-fd6b-4fb3-99d8-3a091011c23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba  \n",
    "import re\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ==========================================\n",
    "# 0. AUTO-INSTALLER \n",
    "# ==========================================\n",
    "def install_and_import_spacy():\n",
    "\n",
    "    try:\n",
    "        import spacy\n",
    "        # Try loading the model to ensure it exists\n",
    "        try:\n",
    "            spacy.load(\"en_core_web_sm\")\n",
    "        except OSError:\n",
    "            print(\"Spacy found, but 'en_core_web_sm' model missing. Downloading...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "            print(\"Model downloaded.\")\n",
    "        return spacy\n",
    "    except ImportError:\n",
    "        print(\"Spacy library not found. Installing now...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"spacy\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "            print(\"Installation complete.\")\n",
    "            import spacy\n",
    "            return spacy\n",
    "        except Exception as e:\n",
    "            print(f\"Automatic installation failed: {e}\")\n",
    "            print(\"Please run these commands in your terminal/command prompt:\")\n",
    "            print(\"  pip install spacy\")\n",
    "            print(\"  python -m spacy download en_core_web_sm\")\n",
    "            sys.exit(1)\n",
    "\n",
    "# Perform the check/install BEFORE importing\n",
    "spacy_nlp = install_and_import_spacy() \n",
    "# load the model\n",
    "nlp_en = spacy_nlp.load(\"en_core_web_sm\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. CLEANING FUNCTIONS \n",
    "# ==========================================\n",
    "\n",
    "def clean_question_text(text):\n",
    "\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    \n",
    "    # A. Standard prefix cleaning\n",
    "    # Removes \"Ask \" at the beginning (case insensitive)\n",
    "    text = re.sub(r'^\\s*ask\\s+', '', text, flags=re.IGNORECASE)\n",
    "    # Removes numbering like \"Q1.\", \"1.\", \"1 \" at the beginning\n",
    "    text = re.sub(r'^\\s*(?:Q\\s*)?\\d+[\\.:\\s]\\s*', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # B. Start with first valid character\n",
    "    # This locates the first English letter or Chinese character\n",
    "    # It cuts off any weird symbols at the start like \"...\" or \"-\"\n",
    "    start_match = re.search(r'[a-zA-Z\\u4e00-\\u9fa5]', text)\n",
    "    if start_match:\n",
    "        text = text[start_match.start():]\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def normalize_for_dedup(text, lang):\n",
    "    \"\"\"\n",
    "    Standardizes text for deduplication (lemmatization + synonym mapping).\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    # Synonym Map: Defining words that mean the same thing\n",
    "    synonym_map = {\n",
    "        \"hotel\": \"accommodation\", \"hotels\": \"accommodation\",\n",
    "        \"inn\": \"accommodation\", \"resort\": \"accommodation\",\n",
    "        \"surveys\": \"survey\", \"travels\": \"travel\",\n",
    "        \"trip\": \"travel\", \"journey\": \"travel\"\n",
    "    }\n",
    "    \n",
    "    if lang == 'en':\n",
    "        # Process English text with Spacy\n",
    "        doc = nlp_en(text)\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            # Skip punctuation and common words (stopwords) like \"the\", \"is\"\n",
    "            if token.is_punct or token.is_stop: continue\n",
    "            \n",
    "            # Get the root word (e.g., \"Traveling\" -> \"Travel\")\n",
    "            word = token.lemma_ \n",
    "            \n",
    "            # Check if this word is in our synonym list and swap it\n",
    "            word = synonym_map.get(word, word)\n",
    "            tokens.append(word)\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    elif lang == 'zh':\n",
    "        # Process Chinese text\n",
    "        # Remove non-Chinese characters\n",
    "        text = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', text)\n",
    "        # Use Jieba to cut the sentence into words\n",
    "        words = jieba.lcut(text)\n",
    "        # Chinese Synonym Map\n",
    "        zh_map = {\"宾馆\": \"酒店\", \"饭店\": \"酒店\", \"住宿\": \"酒店\", \"游览\": \"旅游\"}\n",
    "        return \" \".join([zh_map.get(w, w) for w in words if w.strip()])\n",
    "    \n",
    "    return text\n",
    "\n",
    "# ==========================================\n",
    "# 2. DIFFICULTY SCORING LOGIC\n",
    "# ==========================================\n",
    "\n",
    "def calculate_difficulty(row):\n",
    "\n",
    "    score = 1 # Start with base score 1\n",
    "    \n",
    "    text = str(row.get('question_text', ''))\n",
    "    q_type = str(row.get('question_type', '')).lower()\n",
    "    options = str(row.get('options_text', ''))\n",
    "    lang = row.get('detected_lang', 'en')\n",
    "\n",
    "    # A. TYPE FACTOR\n",
    "    # Open-ended questions are harder because users have to type an answer\n",
    "    if 'open_ended' in q_type:\n",
    "        score += 2\n",
    "    # Multiple choice is slightly harder than simple Yes/No (Base 1)\n",
    "    elif 'multiple_choice' in q_type or 'single_choice' in q_type:\n",
    "        score += 1\n",
    "\n",
    "    # B. LENGTH FACTOR \n",
    "    # Longer questions take more effort to read\n",
    "    if lang == 'en':\n",
    "        word_count = len(text.split())\n",
    "        if word_count > 30: score += 2   \n",
    "        elif word_count > 15: score += 1 \n",
    "    else: \n",
    "        char_count = len(text)\n",
    "        if char_count > 50: score += 2\n",
    "        elif char_count > 20: score += 1\n",
    "\n",
    "    # C. OPTIONS FACTOR\n",
    "    if options:\n",
    "        # estimate number of options by counting separators '/'\n",
    "        option_count = options.count('/') + 1\n",
    "        if option_count > 6: score += 1\n",
    "\n",
    "    # D. WORDING FACTOR\n",
    "    # Check for \"Hard\" keywords that require complex thinking\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    en_hard_words = [\"describe\", \"explain\", \"comprehensive\", \"evaluate\", \"perspective\", \"why\"]\n",
    "    zh_hard_words = [\"描述\", \"解释\", \"详细\", \"评估\", \"看法\", \"为什么\"]\n",
    "    \n",
    "    if lang == 'en':\n",
    "        if any(w in text_lower for w in en_hard_words): score += 1\n",
    "    else:\n",
    "        if any(w in text_lower for w in zh_hard_words): score += 1\n",
    "\n",
    "    # Ensure the score stays between 1 and 5\n",
    "    final_score = max(1, min(5, score))\n",
    "    return final_score\n",
    "\n",
    "# ==========================================\n",
    "# 3. ANALYSIS FUNCTIONS \n",
    "# ==========================================\n",
    "\n",
    "def analyze_dataset_content(df, lang):\n",
    "    print(f\"\\n>>> ANALYSIS FOR {lang.upper()} DATASET <<<\")\n",
    "    \n",
    "    # [1] Question Types Analysis\n",
    "    print(f\"[1] Question Type Distribution:\")\n",
    "    if 'question_type' in df.columns:\n",
    "        # Count unique values in 'question_type' column\n",
    "        type_counts = df['question_type'].fillna('Unknown').value_counts()\n",
    "        for q_type, count in type_counts.items():\n",
    "            print(f\"    - {q_type}: {count}\")\n",
    "    else:\n",
    "        print(\"    (No data)\")\n",
    "\n",
    "    # [2] Topic Coverage Analysis\n",
    "    print(f\"[2] Topic Coverage (Overlapping):\")\n",
    "    \n",
    "    # Define keywords for different topics\n",
    "    if lang == 'en':\n",
    "        topic_keywords = {\n",
    "            \"Hotel/Accommodation\": [\"hotel\", \"accommodation\", \"room\", \"stay\", \"inn\"],\n",
    "            \"Travel/General\":      [\"travel\", \"trip\", \"journey\", \"tour\", \"tourism\"],\n",
    "            \"Flight/Transport\":    [\"flight\", \"airline\", \"plane\", \"transport\", \"bus\"],\n",
    "            \"Food/Dining\":         [\"food\", \"meal\", \"dining\", \"restaurant\", \"eat\"],\n",
    "            \"Service/Satisfaction\":[\"service\", \"staff\", \"satisfaction\", \"quality\"]\n",
    "        }\n",
    "    else:\n",
    "        topic_keywords = {\n",
    "            \"Hotel/Accommodation\": [\"酒店\", \"住宿\", \"房间\", \"宾馆\", \"饭店\"],\n",
    "            \"Travel/General\":      [\"旅游\", \"旅行\", \"行程\", \"度假\"],\n",
    "            \"Flight/Transport\":    [\"航班\", \"飞机\", \"交通\", \"机场\"],\n",
    "            \"Food/Dining\":         [\"餐饮\", \"食物\", \"吃饭\", \"餐厅\"],\n",
    "            \"Service/Satisfaction\":[\"服务\", \"满意\", \"推荐\", \"态度\"]\n",
    "        }\n",
    "\n",
    "    # Initialize counters to 0\n",
    "    topic_counts = {k: 0 for k in topic_keywords}\n",
    "    \n",
    "    # Loop through every question\n",
    "    for text in df['question_text']:\n",
    "        text_lower = str(text).lower()\n",
    "        # Check if the question belongs to a topic (can be multiple topics)\n",
    "        for topic, keywords in topic_keywords.items():\n",
    "            if any(k in text_lower for k in keywords):\n",
    "                topic_counts[topic] += 1\n",
    "\n",
    "    # Print results\n",
    "    for topic, count in topic_counts.items():\n",
    "        print(f\"    - {topic}: {count}\")\n",
    "\n",
    "    # [3] Difficulty Analysis\n",
    "    print(f\"[3] Difficulty Level Distribution (1=Easy, 5=Hard):\")\n",
    "    \n",
    "    # Count how many questions are Level 1, Level 2, etc.\n",
    "    difficulty_counts = df['difficulty_score'].value_counts().sort_index()\n",
    "    \n",
    "    # Loop from 1 to 5 \n",
    "    for i in range(1, 6):\n",
    "        count = difficulty_counts.get(i, 0)\n",
    "        print(f\"    - Level {i}: {count}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# ==========================================\n",
    "# 4. LOADING & PROCESSING PIPELINE\n",
    "# ==========================================\n",
    "\n",
    "def load_and_process_data(filepath):\n",
    "    try:\n",
    "        # Load JSON file\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Convert to DataFrame (Table format)\n",
    "        if isinstance(data, dict) and 'fullContent' in data:\n",
    "            df = pd.DataFrame(data['fullContent'])\n",
    "        else:\n",
    "            df = pd.DataFrame(data)\n",
    "\n",
    "        raw_count = len(df)\n",
    "\n",
    "        # 1. Clean text\n",
    "        # Apply the cleaning function to every row\n",
    "        df['question_text'] = df['question_text'].apply(clean_question_text)\n",
    "        # Remove rows that are too short (empty or junk)\n",
    "        df = df[df['question_text'].str.len() > 2].reset_index(drop=True)\n",
    "\n",
    "        # 2. Detect Language\n",
    "        def detect_lang(text):\n",
    "            # Check for Chinese characters using Unicode range\n",
    "            if re.search(r'[\\u4e00-\\u9fa5]', str(text)): return 'zh'\n",
    "            return 'en'\n",
    "        df['detected_lang'] = df['question_text'].apply(detect_lang)\n",
    "\n",
    "        # 3. Deduplicate\n",
    "        print(\"Normalizing for duplicate detection...\")\n",
    "        # Create a special 'key' that looks effectively the same for similar questions\n",
    "        df['dedup_key'] = df.apply(\n",
    "            lambda x: normalize_for_dedup(str(x['question_text']) + \" \" + str(x.get('options_text', '')), x['detected_lang']), \n",
    "            axis=1\n",
    "        )\n",
    "        # Drop exact duplicates based on this key\n",
    "        df.drop_duplicates(subset=['dedup_key'], keep='first', inplace=True)\n",
    "        \n",
    "        # 4. Calculate Difficulty \n",
    "        # We calculate this BEFORE splitting so the logic applies to all data\n",
    "        df['difficulty_score'] = df.apply(calculate_difficulty, axis=1)\n",
    "\n",
    "        # 5. Split into English and Chinese tables\n",
    "        df_en = df[df['detected_lang'] == 'en'].copy().reset_index(drop=True)\n",
    "        df_zh = df[df['detected_lang'] == 'zh'].copy().reset_index(drop=True)\n",
    "        \n",
    "        # --- DATA REPORT ---\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"          DATA CLEANING REPORT\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"1. Raw Questions Loaded:     {raw_count}\")\n",
    "        print(f\"2. Final Valid Questions:    {len(df)}\")\n",
    "        print(f\"   > English Pool:           {len(df_en)}\")\n",
    "        print(f\"   > Chinese Pool:           {len(df_zh)}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # --- RUN ANALYSIS ---\n",
    "        if not df_en.empty: analyze_dataset_content(df_en, 'en')\n",
    "        if not df_zh.empty: analyze_dataset_content(df_zh, 'zh')\n",
    "\n",
    "        return df_en, df_zh\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "# ==========================================\n",
    "# 5. GENERATION ENGINE (Main Logic)\n",
    "# ==========================================\n",
    "\n",
    "def generate_survey(df, lang, target_count=20):\n",
    "    # Set the input for the normalized text\n",
    "    df['model_input'] = df['dedup_key']\n",
    "\n",
    "    # Initialize the TF-IDF Vectorizer\n",
    "    if lang == 'en':\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    else:\n",
    "        # Chinese requires a custom stopword list\n",
    "        zh_stops = [\"的\", \"了\", \"是\", \"我\", \"你\", \"在\", \"和\", \"有\", \"去\", \"吗\", \"我们\", \"什么\"]\n",
    "        vectorizer = TfidfVectorizer(stop_words=zh_stops)\n",
    "\n",
    "    try:\n",
    "        # Train the model on our data\n",
    "        tfidf_matrix = vectorizer.fit_transform(df['model_input'])\n",
    "    except ValueError:\n",
    "        print(\"Error: Not enough text data.\")\n",
    "        return\n",
    "\n",
    "    # Loop to keep asking for user input\n",
    "    while True:\n",
    "        print(f\"\\n--- Generate {lang.upper()} Survey (Target: {target_count}) \")\n",
    "        print(\"Enter requirement (or type 'exit'):\")\n",
    "        \n",
    "        req = input(\">> \")\n",
    "        \n",
    "        # Check for Exit command\n",
    "        if req.strip().lower() == 'exit':\n",
    "            print(\"Terminating program.\")\n",
    "            sys.exit(0)\n",
    "        \n",
    "        if not req.strip(): continue\n",
    "\n",
    "        # Process user input (Same cleaning as dataset)\n",
    "        processed_req = normalize_for_dedup(req, lang)\n",
    "        \n",
    "        # Convert user input to numbers\n",
    "        req_vector = vectorizer.transform([processed_req])\n",
    "        \n",
    "        # Calculate similarity scores\n",
    "        cosine_sim = cosine_similarity(req_vector, tfidf_matrix).flatten()\n",
    "        \n",
    "        # Sort results from highest score to lowest\n",
    "        sorted_indices = cosine_sim.argsort()[::-1]\n",
    "        \n",
    "        print(f\"\\nResults for: '{req}'\\n\" + \"-\"*40)\n",
    "        \n",
    "        count = 0\n",
    "        seen_text = set()\n",
    "        \n",
    "        # Iterate through the best matches\n",
    "        for idx in sorted_indices:\n",
    "            if count >= target_count: break\n",
    "            \n",
    "            score = cosine_sim[idx]\n",
    "            # Only show results with > 5% similarity\n",
    "            if score > 0.05: \n",
    "                q_text = df.iloc[idx]['question_text']\n",
    "                \n",
    "                # Ensure do not show the same question twice in this list\n",
    "                if q_text not in seen_text:\n",
    "                    # Show Question + Difficulty Score\n",
    "                    diff_score = df.iloc[idx]['difficulty_score']\n",
    "                    \n",
    "                    print(f\"{count + 1}. [Match: {int(score*100)}%] [Diff: {diff_score} {diff_label}]\")\n",
    "                    print(f\"    {q_text}\")\n",
    "                    \n",
    "                    # Show Options if they exist\n",
    "                    opts = df.iloc[idx].get('options_text')\n",
    "                    if pd.notna(opts) and str(opts).strip() != \"\":\n",
    "                        print(f\"    (Options: {opts})\")\n",
    "                    \n",
    "                    print(\"-\" * 40)\n",
    "                    seen_text.add(q_text)\n",
    "                    count += 1\n",
    "                    \n",
    "        if count == 0: print(\"No matches found.\")\n",
    "\n",
    "# ==========================================\n",
    "# MAIN EXECUTION BLOCK (Just for demo what the code doing)\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Path of input file\n",
    "    file_path = r\"C:\\Users\\User\\Desktop\\questions.json\"\n",
    "    \n",
    "    # Step 1: Load and Process Data\n",
    "    english_df, chinese_df = load_and_process_data(file_path)\n",
    "    \n",
    "    if english_df.empty and chinese_df.empty:\n",
    "        print(\"No data loaded. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Step 2: User Menu Loop\n",
    "    while True:\n",
    "        print(\"\\n=========================================\")\n",
    "        print(\"   SURVEY GENERATOR SYSTEM\")\n",
    "        print(\"=========================================\")\n",
    "        print(\"1. Generate English Survey\")\n",
    "        print(\"2. Generate Chinese Survey\")\n",
    "        print(\"Type 'exit' to close program.\")\n",
    "        \n",
    "        choice = input(\"Select: \").strip().lower()\n",
    "        \n",
    "        if choice == '1':\n",
    "            if not english_df.empty: generate_survey(english_df, 'en')\n",
    "            else: print(\"No English questions.\")\n",
    "        elif choice == '2':\n",
    "            if not chinese_df.empty: generate_survey(chinese_df, 'zh')\n",
    "            else: print(\"No Chinese questions.\")\n",
    "        elif choice == 'exit':\n",
    "            print(\"Exiting.\")\n",
    "            sys.exit(0)\n",
    "        else:\n",
    "            print(\"Invalid selection.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
