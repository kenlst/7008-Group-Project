{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a42076c2",
   "metadata": {},
   "source": [
    "# Objective 5: Survey Question Generator (Notebook)\n",
    "\n",
    "This notebook implements the generator design for Objective 5. It provides:\\n\n",
    "- a TF-IDF based retrieval pipeline to match requirements to questions,\\n\n",
    "- a greedy constraint-based selector to satisfy category/difficulty quotas,\\n\n",
    "- optional hooks for sentence-transformer embeddings for improved semantic matching.\n",
    "\n",
    "Notes: The code cells are implementation-ready but not executed here. Follow the 'Next steps' cell later to run locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c113ce9",
   "metadata": {},
   "source": [
    "## Roadmap & Requirements (short)\n",
    "\n",
    "The generator accepts a structured `requirements` object, for example:\\n\n",
    "````python\\nrequirements = {\\n  'question_count': 10,\\n  'categories': ['service','satisfaction'],\\n  'difficulty_range': [1,3],  # inclusive min/max\\n  'lang': 'en'\\n}\\n````\n",
    "\n",
    "Design goals: select unique questions, respect category and difficulty constraints, avoid near-duplicates, and favor high semantic-match to the requirement text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63a6e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and optional dependencies\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# sklearn TF-IDF for baseline semantic retrieval\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Local database API in this repo\n",
    "from survey_database import SurveyDatabase\n",
    "\n",
    "# Optional: sentence-transformers (better semantic matching).\n",
    "# We'll try to import but keep it optional; if not present, TF-IDF will be used.\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    SENTENCE_TRANSFORMER_AVAILABLE = True\n",
    "except Exception:\n",
    "    SentenceTransformer = None\n",
    "    SENTENCE_TRANSFORMER_AVAILABLE = False\n",
    "\n",
    "# Helper: normalize text for vectorization (lightweight)\n",
    "def prepare_text(s: Optional[str]) -> str:\n",
    "    if s is None: return ''\n",
    "    return str(s).strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1794c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a corpus from the database and create vector representations\n",
    "def build_corpus_and_vectorizer(db: SurveyDatabase, lang: str = 'en') -> Tuple[List[Dict[str, Any]], TfidfVectorizer, Any]:\n",
    "    \"\"\"\n",
    "    Returns: (questions_list, vectorizer, matrix_or_embeddings)\n",
    "    - questions_list: list of question dicts (same order as vectors)\n",
    "    - vectorizer: a trained TfidfVectorizer (or None if using embeddings)\n",
    "    - matrix_or_embeddings: TF-IDF matrix or embeddings ndarray\n",
    "    \"\"\"\n",
    "    questions = db.get_all_questions()\n",
    "    # Filter by language if field exists (many items may not have lang recorded)\n",
    "    if lang:\n",
    "        filtered = [q for q in questions if q.get('tags') is None or lang in q.get('tags') or q.get('question_type') ]\n",
    "    else:\n",
    "        filtered = questions\n",
    "\n",
    "    # Create a `text` field used for vectorization\n",
    "    corpus = []\n",
    "    for q in filtered:\n",
    "        text = q.get('question_text') or q.get('text') or ''\n",
    "        opts = q.get('options') or q.get('options_text') or ''\n",
    "        # combine text and options to give weight to option-based questions\n",
    "        combined = prepare_text(text) + ' ' + prepare_text(str(opts))\n",
    "        corpus.append({'q': q, 'text': combined})\n",
    "\n",
    "    texts = [c['text'] for c in corpus]\n",
    "\n",
    "    # Use sentence-transformer embeddings if available for better semantic similarity\n",
    "    if SENTENCE_TRANSFORMER_AVAILABLE:\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        embeddings = model.encode(texts, show_progress_bar=False)\n",
    "        return corpus, None, embeddings\n",
    "\n",
    "    # Fallback to TF-IDF\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    matrix = vectorizer.fit_transform(texts)\n",
    "    return corpus, vectorizer, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25e6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank questions by semantic similarity to the requirement text\n",
    "def rank_questions(corpus, matrix_or_embeddings, vectorizer, requirement_text: str, top_k: int = 200):\n",
    "    req = prepare_text(requirement_text)\n",
    "\n",
    "    if SENTENCE_TRANSFORMER_AVAILABLE and vectorizer is None:\n",
    "        # matrix_or_embeddings is embeddings ndarray\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        req_emb = model.encode([req], show_progress_bar=False)\n",
    "        sims = cosine_similarity(req_emb, matrix_or_embeddings).flatten()\n",
    "    else:\n",
    "        # TF-IDF path\n",
    "        req_vec = vectorizer.transform([req])\n",
    "        sims = cosine_similarity(req_vec, matrix_or_embeddings).flatten()\n",
    "\n",
    "    scored = []\n",
    "    for idx, score in enumerate(sims):\n",
    "        scored.append((idx, float(score)))\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scored[:top_k]\n",
    "\n",
    "# Greedy selector that enforces category & difficulty quotas\n",
    "def select_with_constraints(corpus, scored_indices, question_count: int, categories: Optional[List[str]] = None, difficulty_range: Optional[List[float]] = None):\n",
    "    selected = []\n",
    "    used_texts = set()\n",
    "    cat_counter = Counter()\n",
    "\n",
    "    for idx, score in scored_indices:\n",
    "        if len(selected) >= question_count:\n",
    "            break\n",
    "        q = corpus[idx]['q']\n",
    "        q_text = (q.get('question_text') or q.get('text') or '').strip()\n",
    "\n",
    "        # Skip duplicates by exact text\n",
    "        if q_text in used_texts:\n",
    "            continue\n",
    "\n",
    "        # Category constraint (if provided)\n",
    "        if categories:\n",
    "            q_cat = q.get('category') or q.get('original_category') or 'Uncategorized'\n",
    "            if q_cat not in categories:\n",
    "                continue\n",
    "\n",
    "        # Difficulty constraint (if provided)\n",
    "        if difficulty_range:\n",
    "            diff = q.get('difficulty') or q.get('difficulty_score') or 2\n",
    "            if isinstance(diff, str):\n",
    "                try:\n",
    "                    diff = float(diff)\n",
    "                except Exception:\n",
    "                    diff = 2\n",
    "            if diff < difficulty_range[0] or diff > difficulty_range[1]:\n",
    "                continue\n",
    "\n",
    "        # Passed filters, add to selected set\n",
    "        selected.append({'question': q, 'score': score})\n",
    "        used_texts.add(q_text)\n",
    "        cat_counter[q.get('category') or 'Uncategorized'] += 1\n",
    "\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942d967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-level generation API\n",
    "def generate_questionnaire(requirements: Dict[str, Any], db_path: str = 'convert_data.json') -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    requirements keys:\\n\n",
    "      - question_count (int)\\n\n",
    "      - categories (Optional[List[str]])\\n\n",
    "      - difficulty_range (Optional[List[min,max]])\\n\n",
    "      - lang (Optional[str])\\n\n",
    "      - requirement_text (Optional[str]) -- short natural language description\\n\n",
    "    Returns a dict with 'question_ids' and 'questions' (list of question dicts).\n",
    "    \"\"\"\n",
    "    db = SurveyDatabase(db_path)\n",
    "    lang = requirements.get('lang', 'en')\n",
    "    question_count = int(requirements.get('question_count', 10))\n",
    "    categories = requirements.get('categories')\n",
    "    difficulty_range = requirements.get('difficulty_range')\n",
    "    req_text = requirements.get('requirement_text', '')\n",
    "\n",
    "    corpus, vectorizer, matrix_or_embeddings = build_corpus_and_vectorizer(db, lang=lang)\n",
    "\n",
    "    # If no requirement text provided, prefer highest-usage or random sampling\n",
    "    if not req_text:\n",
    "        # simple fallback: sort by usage_count asc (less used first) or randomize\n",
    "        candidates = [(i, 0.0) for i in range(len(corpus))]\n",
    "    else:\n",
    "        candidates = rank_questions(corpus, matrix_or_embeddings, vectorizer, req_text, top_k=1000)\n",
    "\n",
    "    selected = select_with_constraints(corpus, candidates, question_count, categories, difficulty_range)\n",
    "\n",
    "    # Build return structure\n",
    "    question_ids = []\n",
    "    questions_out = []\n",
    "    for it in selected:\n",
    "        q = it['question']\n",
    "        question_ids.append(q.get('id') or q.get('question_id'))\n",
    "        questions_out.append(q)\n",
    "\n",
    "    return {'question_ids': question_ids, 'questions': questions_out}\n",
    "\n",
    "# Example usage (not executed here)\n",
    "# req = {'question_count': 10, 'categories': ['satisfaction','service'], 'difficulty_range':[1,3], 'lang':'en', 'requirement_text':'measure customer satisfaction after check-in experience'}\n",
    "# result = generate_questionnaire(req, db_path='convert_data.json')\n",
    "# print(result['question_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9758c",
   "metadata": {},
   "source": [
    "## Next steps (what you should run locally)\n",
    "\n",
    "- Install required packages:\\n\n",
    "```bash\\n\n",
    "pip install scikit-learn sentence-transformers\n",
    "```\n",
    "- If you want to use the better semantic matching, install `sentence-transformers`. If not installed, TF-IDF will be used automatically.\n",
    "- Open this notebook in Jupyter or run the `generate_questionnaire` function from a Python script. Example:\\n\n",
    "```python\\n\n",
    "from Q5_Codes_vLST import generate_questionnaire  # if exported as module or copy function to script\\n\n",
    "req = { 'question_count':10, 'categories':['satisfaction'], 'difficulty_range':[1,3], 'lang':'en', 'requirement_text':'post-checkin satisfaction' }\\n\n",
    "res = generate_questionnaire(req, db_path='convert_data.json')\\n\n",
    "print(len(res['questions']))\\n\n",
    "```\n",
    "- Validate the output and add more selection constraints if you need (e.g., avoid overlapping keywords between selected questions).\n",
    "\n",
    "If you want, I can next: implement a stricter deduplication step (embedding-based clustering) and add unit tests that validate category/difficulty quotas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STAT7008",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
